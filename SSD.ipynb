{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMmsDsF4lsyo",
        "outputId": "690253da-c52d-49b2-8ce3-f5fdc5bb2add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/CMPT733-Lab3-Workspace\")"
      ],
      "metadata": {
        "id": "8JTcMf8clvz4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset.py"
      ],
      "metadata": {
        "id": "RdIBokO0l0vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from random import randint\n",
        "\n",
        "#generate default bounding boxes\n",
        "def default_box_generator(layers, large_scale, small_scale):\n",
        "    #input:\n",
        "    #layers      -- a list of sizes of the output layers. in this assignment, it is set to [10,5,3,1].\n",
        "    #large_scale -- a list of sizes for the larger bounding boxes. in this assignment, it is set to [0.2,0.4,0.6,0.8].\n",
        "    #small_scale -- a list of sizes for the smaller bounding boxes. in this assignment, it is set to [0.1,0.3,0.5,0.7].\n",
        "    \n",
        "    #output:\n",
        "    #boxes -- default bounding boxes, shape=[box_num,8]. box_num=4*(10*10+5*5+3*3+1*1) for this assignment.\n",
        "    \n",
        "    #TODO:\n",
        "    #create an numpy array \"boxes\" to store default bounding boxes\n",
        "    #you can create an array with shape [10*10+5*5+3*3+1*1,4,8], and later reshape it to [box_num,8]\n",
        "    #the first dimension means number of cells, 10*10+5*5+3*3+1*1\n",
        "    #the second dimension 4 means each cell has 4 default bounding boxes.\n",
        "    #their sizes are [ssize,ssize], [lsize,lsize], [lsize*sqrt(2),lsize/sqrt(2)], [lsize/sqrt(2),lsize*sqrt(2)],\n",
        "    #where ssize is the corresponding size in \"small_scale\" and lsize is the corresponding size in \"large_scale\".\n",
        "    #for a cell in layer[i], you should use ssize=small_scale[i] and lsize=large_scale[i].\n",
        "    #the last dimension 8 means each default bounding box has 8 attributes: [x_center, y_center, box_width, box_height, x_min, y_min, x_max, y_max]\n",
        "    layer_sq=np.square(layers)\n",
        "    box_num=sum(layer_sq)*len(layers)\n",
        "    box_final=np.zeros((sum(layer_sq),len(layers),8))\n",
        "    table_box_size=np.zeros([sum(layer_sq)])\n",
        "    index_start=np.zeros([len(layers)])\n",
        "    for i in np.arange(len(layers)-1):\n",
        "        index_start[i+1]=index_start[i]+layer_sq[i]\n",
        "    for i in np.arange(len(layers)-1):\n",
        "        table_box_size[int(index_start[i]):int(index_start[i+1])]=layers[i]\n",
        "    table_box_size[int(index_start[len(layers)-1]):len(table_box_size)]=layers[len(layers)-1]\n",
        "    \n",
        "    for i in range(sum(layer_sq)):\n",
        "        size=1.0/table_box_size[i]\n",
        "        layer_index=layers.index(table_box_size[i])\n",
        "        rel_i=i-index_start[layer_index]\n",
        "        ix=rel_i%table_box_size[i]\n",
        "        iy=math.floor(rel_i/table_box_size[i])\n",
        "        for j in range(len(layers)):\n",
        "            box_final[i][j][0]=(ix*size)+size/2\n",
        "            box_final[i][j][1]=(iy*size)+size/2\n",
        "            if j==0:\n",
        "                height=width=small_scale[layer_index]\n",
        "            elif j==1:\n",
        "                height=width=large_scale[layer_index]\n",
        "            elif j==2:\n",
        "                height=large_scale[layer_index]/math.sqrt(2)\n",
        "                width=min(height,1)\n",
        "            elif j==3:\n",
        "                width=large_scale[layer_index]/math.sqrt(2)\n",
        "                height=min(width,1)\n",
        "            box_final[i][j][2]=width\n",
        "            box_final[i][j][3]=height\n",
        "            box_final[i][j][4]=max(box_final[i][j][0]-width/2,0)\n",
        "            box_final[i][j][5]=max(box_final[i][j][1]-height/2,0)\n",
        "            box_final[i][j][6]=min(box_final[i][j][0]+width/2,1)\n",
        "            box_final[i][j][7]=min(box_final[i][j][1]+height/2,1) \n",
        "    box_final=box_final.reshape((box_num,8))\n",
        "    return box_final\n",
        "\n",
        "#this is an example implementation of IOU.\n",
        "#It is different from the one used in YOLO, please pay attention.\n",
        "#you can define your own iou function if you are not used to the inputs of this one.\n",
        "def iou(boxs_default, x_min,y_min,x_max,y_max):\n",
        "    #input:\n",
        "    #boxes -- [num_of_boxes, 8], a list of boxes stored as [box_1,box_2, ...], where box_1 = [x1_center, y1_center, width, height, x1_min, y1_min, x1_max, y1_max].\n",
        "    #x_min,y_min,x_max,y_max -- another box (box_r)\n",
        "    \n",
        "    #output:\n",
        "    #ious between the \"boxes\" and the \"another box\": [iou(box_1,box_r), iou(box_2,box_r), ...], shape = [num_of_boxes]\n",
        "    inter = np.maximum(np.minimum(boxs_default[:,6],x_max)-np.maximum(boxs_default[:,4],x_min),0)*np.maximum(np.minimum(boxs_default[:,7],y_max)-np.maximum(boxs_default[:,5],y_min),0)\n",
        "    area_a = (boxs_default[:,6]-boxs_default[:,4])*(boxs_default[:,7]-boxs_default[:,5])\n",
        "    area_b = (x_max-x_min)*(y_max-y_min)\n",
        "    union = area_a + area_b - inter\n",
        "    return inter/np.maximum(union,1e-8)\n",
        "\n",
        "def match(ann_box,ann_confidence,boxs_default,threshold,cat_id,x_min,y_min,x_max,y_max):\n",
        "    #input:\n",
        "    #ann_box                 -- [num_of_boxes,4], ground truth bounding boxes to be updated\n",
        "    #ann_confidence          -- [num_of_boxes,number_of_classes], ground truth class labels to be updated\n",
        "    #boxs_default            -- [num_of_boxes,8], default bounding boxes\n",
        "    #threshold               -- if a default bounding box and the ground truth bounding box have iou>threshold, then this default bounding box will be used as an anchor\n",
        "    #cat_id                  -- class id, 0-cat, 1-dog, 2-person\n",
        "    #x_min,y_min,x_max,y_max -- bounding box\n",
        "\n",
        "    #compute iou between the default bounding boxes and the ground truth bounding box\n",
        "    ious = iou(boxs_default, x_min,y_min,x_max,y_max)\n",
        "    ious_true = ious>threshold\n",
        "    #TODO:\n",
        "    #update ann_box and ann_confidence, with respect to the ious and the default bounding boxes.\n",
        "    #if a default bounding box and the ground truth bounding box have iou>threshold, then we will say this default bounding box is carrying an object.\n",
        "    #this default bounding box will be used to update the corresponding entry in ann_box and ann_confidence\n",
        "    idx = [np.argmax(ious)]\n",
        "    #TODO:\n",
        "    #make sure at least one default bounding box is used\n",
        "    #update ann_box and ann_confidence (do the same thing as above)\n",
        "\n",
        "    gw=x_max-x_min\n",
        "    gh=y_max-y_min\n",
        "    gx=x_min+gw/2\n",
        "    gy=y_min+gh/2\n",
        "    px=boxs_default[idx,0]\n",
        "    py=boxs_default[idx,1]\n",
        "    pw=boxs_default[idx,2]\n",
        "    ph=boxs_default[idx,3]\n",
        "    tx=(gx-px)/pw\n",
        "    ty=(gy-py)/ph\n",
        "    tw=np.log(gw/pw)\n",
        "    th=np.log(gh/ph)\n",
        "    ann_box[idx,0]=tx\n",
        "    ann_box[idx,1]=ty\n",
        "    ann_box[idx,2]=tw\n",
        "    ann_box[idx,3]=th\n",
        "    ann_confidence[idx,cat_id] = 1\n",
        "    ann_confidence[idx,-1] = 0\n",
        "\n",
        "class COCO(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgdir, anndir, class_num, boxs_default, train = True, augmentation = True, split = True, image_size=320):\n",
        "        self.train = train\n",
        "        self.augmentation = augmentation\n",
        "        self.split = split\n",
        "        self.imgdir = imgdir\n",
        "        self.anndir = anndir\n",
        "        self.class_num = class_num\n",
        "        \n",
        "        #overlap threshold for deciding whether a bounding box carries an object or no\n",
        "        self.threshold = 0.5\n",
        "        self.boxs_default = boxs_default\n",
        "        self.box_num = len(self.boxs_default)\n",
        "        \n",
        "        self.img_names = os.listdir(self.imgdir)\n",
        "        self.img_names.sort()\n",
        "        self.image_size = image_size\n",
        "        \n",
        "        #notice:\n",
        "        #you can split the dataset into 90% training and 10% validation here, by slicing self.img_names with respect to self.train\n",
        "        ratio = 0.9\n",
        "        ttl = len(self.img_names)\n",
        "        if self.train == True and self.split == True:\n",
        "            self.img_names = self.img_names[0:round(ttl*ratio)]\n",
        "        elif self.train == False and self.split == True:\n",
        "            self.img_names = self.img_names[round(ttl*ratio):ttl]\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train == True and self.augmentation == True:\n",
        "            return len(self.img_names)*2\n",
        "        else:\n",
        "            return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ann_box = np.zeros([self.box_num,4], np.float32) #bounding boxes\n",
        "        ann_confidence = np.zeros([self.box_num,self.class_num], np.float32) #one-hot vectors\n",
        "        #one-hot vectors with four classes\n",
        "        #[1,0,0,0] -> cat\n",
        "        #[0,1,0,0] -> dog\n",
        "        #[0,0,1,0] -> person\n",
        "        #[0,0,0,1] -> background\n",
        "\n",
        "        ann_confidence[:,-1] = 1 #the default class for all cells is set to \"background\"\n",
        "        if self.train == True and self.augmentation == True:\n",
        "            if index % 2 == 1:\n",
        "                i = int((index-1)/2)\n",
        "            else:\n",
        "                i = int(index/2)\n",
        "        else:\n",
        "            i = index\n",
        "        img_name = self.imgdir+self.img_names[i]\n",
        "        if self.anndir != None:\n",
        "            ann_name = self.anndir+self.img_names[i][:-3]+\"txt\"\n",
        "        filename = self.img_names[i][:-4]\n",
        "        im = cv2.imread(img_name)\n",
        "        shape = im.shape\n",
        "        if self.anndir == None:\n",
        "            im = cv2.resize(im, (self.image_size, self.image_size), interpolation = cv2.INTER_AREA)\n",
        "            im = im.transpose(2,0,1)\n",
        "            #im = np.swapaxes(im,2,1)\n",
        "            #im = np.swapaxes(im,1,0)\n",
        "            im = (im/255.0).astype('float32')\n",
        "            return im, ann_box, ann_confidence, torch.Tensor(shape),filename\n",
        "\n",
        "        with open(ann_name) as f:\n",
        "            line = f.read().splitlines()\n",
        "        n_file = len(line) \n",
        "        x_min = np.zeros(n_file)\n",
        "        y_min = np.zeros(n_file)\n",
        "        x_max = np.zeros(n_file)\n",
        "        y_max = np.zeros(n_file)\n",
        "        \n",
        "        for i in range(0,n_file):\n",
        "            data = line[i].split(\" \")\n",
        "            cat_id = int(data[0])\n",
        "            x_min[i] = float(data[1])\n",
        "            y_min[i] = float(data[2])\n",
        "            width = float(data[3])\n",
        "            height = float(data[4])\n",
        "            x_max[i] = x_min[i] + width\n",
        "            y_max[i] = y_min[i] + height\n",
        "        if self.train == True and self.augmentation == True:\n",
        "            if index % 2 == 1:\n",
        "                # Random Crop\n",
        "                X_min = min(x_min)\n",
        "                X_max = max(x_max)\n",
        "                Y_min = min(y_min)\n",
        "                Y_max = max(y_max)\n",
        "                left = randint(0, math.floor(X_min))\n",
        "                right = randint(math.ceil(X_max), shape[1])\n",
        "                top = randint(0, math.floor(Y_min))\n",
        "                bottom = randint(math.ceil(Y_max), shape[0])\n",
        "                aug = im[top:bottom,left:right]\n",
        "                x_min = x_min - left\n",
        "                x_max = x_max - left\n",
        "                y_min = y_min - top\n",
        "                y_max = y_max - top\n",
        "                shape = aug.shape\n",
        "                \n",
        "                # Flip\n",
        "                aug = np.flip(aug, axis=1)\n",
        "                tmp = x_min\n",
        "                x_min = shape[1] - x_max\n",
        "                x_max = shape[1] - tmp\n",
        "\n",
        "                im = cv2.resize(aug, (self.image_size, self.image_size), interpolation = cv2.INTER_AREA)\n",
        "                im = im.transpose(2,0,1)\n",
        "            else:\n",
        "                im = cv2.resize(im, (self.image_size, self.image_size), interpolation = cv2.INTER_AREA)\n",
        "                im = im.transpose(2,0,1)\n",
        "        else:\n",
        "            im = cv2.resize(im, (self.image_size, self.image_size), interpolation = cv2.INTER_AREA)\n",
        "            im = im.transpose(2,0,1)\n",
        "        for i in range(0,n_file):\n",
        "            x_min[i] = x_min[i] / shape[1]\n",
        "            y_min[i] = y_min[i] / shape[0]\n",
        "            x_max[i] = x_max[i] / shape[1]\n",
        "            y_max[i] = y_max[i] / shape[0]\n",
        "            match(ann_box,ann_confidence,self.boxs_default,self.threshold,cat_id,x_min[i],y_min[i],x_max[i],y_max[i])\n",
        "        im = (im/255.0).astype('float32')\n",
        "        \n",
        "        return im, ann_box, ann_confidence, torch.Tensor(shape), filename"
      ],
      "metadata": {
        "id": "Ujn7F01QlzL-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model.py"
      ],
      "metadata": {
        "id": "6mkXVCPC0gO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def SSD_loss(pred_confidence, pred_box, ann_confidence, ann_box):\n",
        "    #input:\n",
        "    #pred_confidence -- the predicted class labels from SSD, [batch_size, num_of_boxes, num_of_classes]\n",
        "    #pred_box        -- the predicted bounding boxes from SSD, [batch_size, num_of_boxes, 4]\n",
        "    #ann_confidence  -- the ground truth class labels, [batch_size, num_of_boxes, num_of_classes]\n",
        "    #ann_box         -- the ground truth bounding boxes, [batch_size, num_of_boxes, 4]\n",
        "    #\n",
        "    #output:\n",
        "    #loss -- a single number for the value of the loss function, [1]\n",
        "    \n",
        "    #TODO: write a loss function for SSD\n",
        "    #\n",
        "    #For confidence (class labels), use cross entropy (F.cross_entropy)\n",
        "    #You can try F.binary_cross_entropy and see which loss is better\n",
        "    #For box (bounding boxes), use smooth L1 (F.smooth_l1_loss)\n",
        "    #\n",
        "    #Note that you need to consider cells carrying objects and empty cells separately.\n",
        "    #I suggest you to reshape confidence to [batch_size*num_of_boxes, num_of_classes]\n",
        "    #and reshape box to [batch_size*num_of_boxes, 4].\n",
        "    #Then you need to figure out how you can get the indices of all cells carrying objects,\n",
        "    #and use confidence[indices], box[indices] to select those cells.\n",
        "\n",
        "    sz = ann_confidence.shape[0]*ann_confidence.shape[1]\n",
        "    pred_confidence = pred_confidence.reshape(sz, ann_confidence.shape[2])\n",
        "    pred_box = pred_box.reshape(sz, 4)\n",
        "    ann_confidence = ann_confidence.reshape(sz, ann_confidence.shape[2])\n",
        "    ann_box = ann_box.reshape(sz, 4)\n",
        "    \n",
        "    _, cls = torch.max(ann_confidence, 1)\n",
        "    obj = (cls != 3).nonzero()\n",
        "    noobj = (cls == 3).nonzero()\n",
        "    obj = obj.reshape(len(obj))\n",
        "    noobj = noobj.reshape(len(noobj))\n",
        "    pred_conf_obj = pred_confidence[obj]\n",
        "    pred_box_obj = pred_box[obj]\n",
        "    ann_conf_obj = ann_confidence[obj]\n",
        "    ann_box_obj = ann_box[obj]\n",
        "    pred_conf_noobj = pred_confidence[noobj]\n",
        "    ann_conf_noobj = ann_confidence[noobj]\n",
        "    \n",
        "    l_cls_obj = F.binary_cross_entropy(pred_conf_obj, ann_conf_obj)\n",
        "    l_cls_noobj = 3*F.binary_cross_entropy(pred_conf_noobj, ann_conf_noobj)\n",
        "    l_box = F.smooth_l1_loss(pred_box_obj, ann_box_obj)\n",
        "    loss = l_cls_obj + l_cls_noobj + l_box\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def conv_layer(in_channel, out_channel, ker_sz, s, p=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channel, out_channel, kernel_size=ker_sz, stride=s, padding=p),\n",
        "        nn.BatchNorm2d(out_channel),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "class conv_reshape(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "        super(conv_reshape, self).__init__()\n",
        "        self.conv = nn.Conv2d(256, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.shape = shape\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(len(x),16,self.shape)\n",
        "        return x\n",
        "\n",
        "def permute(x):\n",
        "    x = x.permute(0,2,1)\n",
        "    return x\n",
        "\n",
        "class SSD(nn.Module):\n",
        "\n",
        "    def __init__(self, class_num):\n",
        "        super(SSD, self).__init__()\n",
        "        \n",
        "        self.class_num = class_num #num_of_classes, in this assignment, 4: cat, dog, person, background\n",
        "        \n",
        "        #TODO: define layers\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.layer.append(conv_layer(3, 64, 3, 2))\n",
        "        self.layer.append(conv_layer(64, 64, 3, 1))\n",
        "        self.layer.append(conv_layer(64, 64, 3, 1))\n",
        "        self.layer.append(conv_layer(64, 128, 3, 2))\n",
        "        self.layer.append(conv_layer(128, 128, 3, 1))\n",
        "        self.layer.append(conv_layer(128, 128, 3, 1))\n",
        "        self.layer.append(conv_layer(128, 256, 3, 2))\n",
        "        self.layer.append(conv_layer(256, 256, 3, 1))\n",
        "        self.layer.append(conv_layer(256, 256, 3, 1))\n",
        "        self.layer.append(conv_layer(256, 512, 3, 2))\n",
        "        self.layer.append(conv_layer(512, 512, 3, 1))\n",
        "        self.layer.append(conv_layer(512, 512, 3, 1))\n",
        "        self.layer.append(conv_layer(512, 256, 3, 2))\n",
        "        self.layer.append(conv_layer(256, 256, 1, 1, 0))\n",
        "        self.layer.append(conv_layer(256, 256, 3, 2))\n",
        "        self.layer.append(conv_layer(256, 256, 1, 1, 0))\n",
        "        self.layer.append(conv_layer(256, 256, 3, 1, 0))\n",
        "        self.layer.append(conv_layer(256, 256, 1, 1, 0))\n",
        "        self.layer.append(conv_layer(256, 256, 3, 1, 0))\n",
        "\n",
        "        self.path1l = conv_reshape(100)\n",
        "        self.path1r = conv_reshape(100)\n",
        "        self.path2l = conv_reshape(25)\n",
        "        self.path2r = conv_reshape(25)\n",
        "        self.path3l = conv_reshape(9)\n",
        "        self.path3r = conv_reshape(9)\n",
        "        self.path4l = nn.Conv2d(256, 16, kernel_size=1, stride=1, padding=0)\n",
        "        self.path4r = nn.Conv2d(256, 16, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "    def forward(self, x):        \n",
        "        #input:\n",
        "        #x -- images, [batch_size, 3, 320, 320]\n",
        "        \n",
        "        x = x/255.0 #normalize image. If you already normalized your input image in the dataloader, remove this line.\n",
        "        \n",
        "        #TODO: define forward\n",
        "        \n",
        "        #should you apply softmax to confidence? (search the pytorch tutorial for F.cross_entropy.) If yes, which dimension should you apply softmax?\n",
        "        \n",
        "        #sanity check: print the size/shape of the confidence and bboxes, make sure they are as follows:\n",
        "        #confidence - [batch_size,4*(10*10+5*5+3*3+1*1),num_of_classes]\n",
        "        #bboxes - [batch_size,4*(10*10+5*5+3*3+1*1),4]\n",
        "        for i in range(13):\n",
        "            x = self.layer[i](x)\n",
        "        path1l = self.path1l(x)\n",
        "        path1r = self.path1r(x)\n",
        "        \n",
        "        x = self.layer[13](x)\n",
        "        x = self.layer[14](x)\n",
        "        path2l = self.path2l(x)\n",
        "        path2r = self.path2r(x)\n",
        "        \n",
        "        x = self.layer[15](x)\n",
        "        x = self.layer[16](x)\n",
        "        path3l = self.path3l(x)\n",
        "        path3r = self.path3r(x)\n",
        "        \n",
        "        x = self.layer[17](x)\n",
        "        x = self.layer[18](x)\n",
        "        path4l = self.path4l(x).reshape(len(x),16,1)\n",
        "        path4r = self.path4r(x).reshape(len(x),16,1)\n",
        "        \n",
        "        left = permute(torch.cat([path1l,path2l,path3l,path4l],dim=2)).reshape(len(x), 540, 4)\n",
        "        right = permute(torch.cat([path1r,path2r,path3r,path4r], dim=2)).reshape(len(x), 540, self.class_num)\n",
        "        right = torch.softmax(right,dim=2)\n",
        "       \n",
        "        bboxes = left\n",
        "        confidence = right\n",
        "        \n",
        "        return confidence,bboxes"
      ],
      "metadata": {
        "id": "Yiu_50dy0hrh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utils.py"
      ],
      "metadata": {
        "id": "2cPZYuPR0jS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "\n",
        "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "#use [blue green red] to represent different classes\n",
        "\n",
        "def visualize_pred(windowname, pred_confidence, pred_box, ann_confidence, ann_box, image_, boxs_default):\n",
        "    #input:\n",
        "    #windowname      -- the name of the window to display the images\n",
        "    #pred_confidence -- the predicted class labels from SSD, [num_of_boxes, num_of_classes]\n",
        "    #pred_box        -- the predicted bounding boxes from SSD, [num_of_boxes, 4]\n",
        "    #ann_confidence  -- the ground truth class labels, [num_of_boxes, num_of_classes]\n",
        "    #ann_box         -- the ground truth bounding boxes, [num_of_boxes, 4]\n",
        "    #image_          -- the input image to the network\n",
        "    #boxs_default    -- default bounding boxes, [num_of_boxes, 8]\n",
        "\n",
        "    _, class_num = pred_confidence.shape\n",
        "    #class_num = 4\n",
        "    class_num = class_num-1\n",
        "    #class_num = 3 now, because we do not need the last class (background)\n",
        "    \n",
        "    image_ = (image_*255).astype(np.uint8)\n",
        "    image = np.transpose(image_, (1,2,0)).astype(np.uint8)\n",
        "    shape = image.shape\n",
        "    image1 = np.zeros(image.shape,np.uint8)\n",
        "    image2 = np.zeros(image.shape,np.uint8)\n",
        "    image3 = np.zeros(image.shape,np.uint8)\n",
        "    image4 = np.zeros(image.shape,np.uint8)\n",
        "    image1[:]=image[:]\n",
        "    image2[:]=image[:]\n",
        "    image3[:]=image[:]\n",
        "    image4[:]=image[:]\n",
        "    #image1: draw ground truth bounding boxes on image1\n",
        "    #image2: draw ground truth \"default\" boxes on image2 (to show that you have assigned the object to the correct cell/cells)\n",
        "    #image3: draw network-predicted bounding boxes on image3\n",
        "    #image4: draw network-predicted \"default\" boxes on image4 (to show which cell does your network think that contains an object)\n",
        "    \n",
        "    #draw ground truth\n",
        "    for i in range(len(ann_confidence)):\n",
        "        for j in range(class_num):\n",
        "            if ann_confidence[i,j]>0.5:\n",
        "                #TODO:\n",
        "                #image1: draw ground truth bounding boxes on image1\n",
        "                #image2: draw ground truth \"default\" boxes on image2 (to show that you have assigned the object to the correct cell/cells)\n",
        "                \n",
        "                #you can use cv2.rectangle as follows:\n",
        "                #start_point = (x1, y1) #top left corner, x1<x2, y1<y2\n",
        "                #end_point = (x2, y2) #bottom right corner\n",
        "                #color = colors[j] #use red green blue to represent different classes\n",
        "                #thickness = 2\n",
        "                #cv2.rectangle(image?, start_point, end_point, color, thickness)\n",
        "\n",
        "                color = colors[j]\n",
        "                thickness = 2\n",
        "                x1,y1,x2,y2,_,_ = box_coordination(ann_box[i], shape, boxs_default[i])\n",
        "                start_point = (x1, y1)\n",
        "                end_point = (x2, y2)\n",
        "                cv2.rectangle(image1, start_point, end_point, color, thickness)\n",
        "                x1 = int(boxs_default[i,4] * shape[1])\n",
        "                y1 = int(boxs_default[i,5] * shape[0])\n",
        "                x2 = int(boxs_default[i,6] * shape[1])\n",
        "                y2 = int(boxs_default[i,7] * shape[0])\n",
        "                start_point = (x1, y1)\n",
        "                end_point = (x2, y2)\n",
        "                cv2.rectangle(image2, start_point, end_point, color, thickness)\n",
        "    \n",
        "    for i in range(len(pred_confidence)):\n",
        "        for j in range(class_num):\n",
        "            if pred_confidence[i,j]>0.5:\n",
        "                #TODO:\n",
        "                #image3: draw network-predicted bounding boxes on image3\n",
        "                #image4: draw network-predicted \"default\" boxes on image4 (to show which cell does your network think that contains an object)\n",
        "                color = colors[j]\n",
        "                thickness = 2\n",
        "                x1,y1,x2,y2,_,_ = box_coordination(pred_box[i], shape, boxs_default[i])\n",
        "                start_point = (x1, y1)\n",
        "                end_point = (x2, y2)\n",
        "                cv2.rectangle(image3, start_point, end_point, color, thickness)\n",
        "                x1 = int(boxs_default[i,4] * shape[1])\n",
        "                y1 = int(boxs_default[i,5] * shape[0])\n",
        "                x2 = int(boxs_default[i,6] * shape[1])\n",
        "                y2 = int(boxs_default[i,7] * shape[0])\n",
        "                start_point = (x1, y1)\n",
        "                end_point = (x2, y2)\n",
        "                cv2.rectangle(image4, start_point, end_point, color, thickness)\n",
        "\n",
        "    #combine four images into one\n",
        "    h,w,_ = image1.shape\n",
        "    image = np.zeros([h*2,w*2,3], np.uint8)\n",
        "    image[:h,:w] = image1\n",
        "    image[:h,w:] = image2\n",
        "    image[h:,:w] = image3\n",
        "    image[h:,w:] = image4\n",
        "    # cv2.imshow(windowname+\" [[gt_box,gt_dft],[pd_box,pd_dft]]\",image)\n",
        "    # cv2.waitKey(1)\n",
        "    #if you are using a server, you may not be able to display the image.\n",
        "    #in that case, please save the image using cv2.imwrite and check the saved image for visualization.\n",
        "    return image\n",
        "\n",
        "def non_maximum_suppression(confidence, box, boxs_default, overlap=0.2, threshold=0.5):\n",
        "\n",
        "    #input:\n",
        "    #confidence_  -- the predicted class labels from SSD, [num_of_boxes, num_of_classes]\n",
        "    #box_         -- the predicted bounding boxes from SSD, [num_of_boxes, 4]\n",
        "    #boxs_default -- default bounding boxes, [num_of_boxes, 8]\n",
        "    #overlap      -- if two bounding boxes in the same class have iou > overlap, then one of the boxes must be suppressed\n",
        "    #threshold    -- if one class in one cell has confidence > threshold, then consider this cell carrying a bounding box with this class.\n",
        "    \n",
        "    #output:\n",
        "    #depends on your implementation.\n",
        "    #if you wish to reuse the visualize_pred function above, you need to return a \"suppressed\" version of confidence [5,5, num_of_classes].\n",
        "    #you can also directly return the final bounding boxes and classes, and write a new visualization function for that.\n",
        "    \n",
        "    \n",
        "    #TODO: non maximum suppression\n",
        "\n",
        "    N = len(boxs_default)\n",
        "    cls_num = confidence.shape[1]\n",
        "    a_box = np.zeros([N,4])\n",
        "    a_conf = np.zeros([N,cls_num])\n",
        "    a_conf[:,-1] = 1\n",
        "    b_box = np.zeros([N,4])\n",
        "    b_conf = np.zeros([N,cls_num])\n",
        "    b_conf[:,-1] = 1\n",
        "    \n",
        "    idx, cat = np.where(confidence[:,0:3] > threshold)\n",
        "    a_conf[idx] = confidence[idx]\n",
        "    a_box[idx] = box[idx]\n",
        "    dx = box[:,0]\n",
        "    dy = box[:,1]\n",
        "    dw = box[:,2]\n",
        "    dh = box[:,3]\n",
        "    px = boxs_default[:,0]\n",
        "    py = boxs_default[:,1]\n",
        "    pw = boxs_default[:,2]\n",
        "    ph = boxs_default[:,3]\n",
        "    gx = pw * dx + px\n",
        "    gy = ph * dy + py\n",
        "    gw = pw * np.exp(dw)\n",
        "    gh = ph * np.exp(dh)\n",
        "    x0 = gx\n",
        "    y0 = gy\n",
        "    width = gw\n",
        "    height = gh\n",
        "    \n",
        "    loc = np.zeros((len(a_box),8))\n",
        "    for i in range(len(boxs_default)):\n",
        "        loc[i,0] = x0[i]\n",
        "        loc[i,1] = y0[i]\n",
        "        loc[i,2] = width[i]\n",
        "        loc[i,3] = height[i]\n",
        "        loc[i,4] = x0[i] - (width[i]/2)\n",
        "        loc[i,5] = y0[i] - (height[i]/2)\n",
        "        loc[i,6] = x0[i] + (width[i]/2)\n",
        "        loc[i,7] = y0[i] + (height[i]/2)\n",
        "    while (np.max(a_conf[:,0:3]) > threshold):\n",
        "        candidate = np.argmax(a_conf[:,0:3])\n",
        "        idx, cat = np.unravel_index(candidate, [N, 3])\n",
        "        b_conf[idx,:] = a_conf[idx,:]\n",
        "        b_box[idx,:] = a_box[idx,:]\n",
        "        a_conf[idx,:] = [0,0,0,1]\n",
        "        a_box[idx,:] = [0,0,0,0]\n",
        "        x_min = loc[idx,4]\n",
        "        y_min = loc[idx,5]\n",
        "        x_max = loc[idx,6]\n",
        "        y_max = loc[idx,7]\n",
        "\n",
        "        overlapping = np.where(a_conf[:,0:3] > threshold)[0]\n",
        "        iou_val = iou(loc[overlapping], x_min,y_min,x_max,y_max)\n",
        "        idx_overlap = np.where(iou_val > overlap)[0]\n",
        "        idx_remove = overlapping[idx_overlap]\n",
        "        a_conf[idx_remove,:] = [0,0,0,1]\n",
        "        a_box[idx_remove,:] = [0,0,0,0]\n",
        "\n",
        "    return b_conf, b_box\n",
        "\n",
        "\n",
        "def box_coordination(box, shape, boxs_default):\n",
        "    dx = box[0]\n",
        "    dy = box[1]\n",
        "    dw = box[2]\n",
        "    dh = box[3]\n",
        "    px = boxs_default[0]\n",
        "    py = boxs_default[1]\n",
        "    pw = boxs_default[2]\n",
        "    ph = boxs_default[3]\n",
        "    gx = pw*dx+px\n",
        "    gy = ph*dy+py\n",
        "    gw = pw*math.exp(dw)\n",
        "    gh = ph*math.exp(dh)\n",
        "    x0 = gx*shape[1]\n",
        "    y0 = gy*shape[0]\n",
        "\n",
        "    width = gw*shape[1]\n",
        "    height = gh*shape[0]\n",
        "    x1 = int(x0-(width/2))\n",
        "    y1 = int(y0-(height/2))\n",
        "    x2 = int(x0+(width/2))\n",
        "    y2 = int(y0+(height/2))\n",
        "    \n",
        "    return x1,y1,x2,y2,width,height    \n",
        "    \n",
        "def visualize(idx, windowname, pred_confidence, pred_box, ann_confidence, ann_box, im, boxs_default, toFile=False, pathname=\"\"):\n",
        "    if torch.is_tensor(pred_confidence):\n",
        "        pred_confidence = pred_confidence[idx].detach().cpu().numpy()\n",
        "    if torch.is_tensor(pred_box):\n",
        "        pred_box = pred_box[idx].detach().cpu().numpy()\n",
        "    if torch.is_tensor(ann_confidence):\n",
        "        ann_confidence = ann_confidence[idx].detach().cpu().numpy()\n",
        "    if torch.is_tensor(ann_box):\n",
        "        ann_box = ann_box[idx].detach().cpu().numpy()\n",
        "    if torch.is_tensor(im):\n",
        "        im = im[idx].detach().cpu().numpy()\n",
        "    im = visualize_pred(windowname, pred_confidence, pred_box, ann_confidence, ann_box, im, boxs_default)\n",
        "    if toFile == True:\n",
        "        cv2.imwrite(pathname + \".jpeg\", im)\n",
        "    else:\n",
        "        plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def toTxt(train, name, iteration, pred_confidence, pred_box, shape, batch_size, boxs_default):\n",
        "    if train == True:\n",
        "        path = \"predicted_boxes/train/\"\n",
        "    else:\n",
        "        path = \"predicted_boxes/test/\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    \n",
        "    if torch.is_tensor(pred_box):\n",
        "        pred_box = pred_box.detach().cpu().numpy()\n",
        "    if torch.is_tensor(pred_confidence):\n",
        "        pred_confidence = pred_confidence.detach().cpu().numpy()\n",
        "    if pred_box.ndim == 2:\n",
        "        pred_box = np.reshape(pred_box, (batch_size, pred_box.shape[0], pred_box.shape[1]))\n",
        "    if pred_confidence.ndim == 2:\n",
        "        pred_confidence = np.reshape(pred_confidence, (batch_size, pred_confidence.shape[0], pred_confidence.shape[1]))\n",
        "\n",
        "    for i in range(len(pred_box)):\n",
        "        filename = os.path.join(path, str(name[i])+'.txt')\n",
        "        with open(filename,\"w\") as f:\n",
        "            idx, cls = np.where(pred_confidence[i,:,0:3] > 0.5)\n",
        "            num_obj = len(idx)\n",
        "            for j in range(num_obj):\n",
        "                index = idx[j]\n",
        "                cat_id = cls[j]\n",
        "                x1,y1,_,_,width,height = box_coordination(pred_box[i,index], shape[i].numpy(), boxs_default[index])\n",
        "                content = str(cat_id) +' '+ str(float(x1)) +' '+ str(float(y1)) +' '+ str(float(width)) +' '+ str(float(height)) + '\\n'\n",
        "                f.write(content)"
      ],
      "metadata": {
        "id": "giu_RBkK0l17"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py"
      ],
      "metadata": {
        "id": "DkPQIlO20nrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "class_num = 4\n",
        "\n",
        "num_epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "boxs_default = default_box_generator([10,5,3,1], [0.2,0.4,0.6,0.8], [0.1,0.3,0.5,0.7])\n",
        "\n",
        "network = SSD(class_num)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "network.to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "saved = 0\n",
        "cpts = 'checkpoints/network'\n",
        "ext = '.pth'\n",
        "_checkpoint = cpts+str(saved)+ext\n",
        "_result = \"results/\"\n",
        "_dir = 'train'\n",
        "_TEST = False\n",
        "\n",
        "if not _TEST:\n",
        "    dataset = COCO(\"data/train/images/\", \"data/train/annotations/\", class_num, boxs_default, train = True, augmentation = True, split = True, image_size=320)\n",
        "    dataset_test = COCO(\"data/train/images/\", \"data/train/annotations/\", class_num, boxs_default, train = False, augmentation = False, split = True, image_size=320)\n",
        "    \n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    \n",
        "    optimizer = optim.Adam(network.parameters(), lr = 1e-4)\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if os.path.exists(_checkpoint):\n",
        "        network.load_state_dict(torch.load(_checkpoint,map_location=torch.device(device)))\n",
        "        print(\"Loaded model \"+_checkpoint+\" to resume training\")\n",
        "    \n",
        "    train_loss=[]\n",
        "    validation_loss=[]\n",
        "    for epoch in range(num_epochs):\n",
        "        network.train()\n",
        "        ttl_loss = 0\n",
        "        item_count = 0\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            images_, ann_box_, ann_confidence_, shape, _ = data\n",
        "            images = images_.to(device)\n",
        "            ann_box = ann_box_.to(device)\n",
        "            ann_confidence = ann_confidence_.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            pred_confidence, pred_box = network(images)\n",
        "\n",
        "            loss_net = SSD_loss(pred_confidence, pred_box, ann_confidence, ann_box)\n",
        "            loss_net.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            ttl_loss += loss_net.data\n",
        "            item_count += 1            \n",
        "            print('\\rTraining: %d\\t' % (i+1), end=\"\")\n",
        "            print(ttl_loss / item_count, end=\"\")\n",
        "        \n",
        "        print('\\r[%d] time: %f \\ttrain loss: %f\\t\\t\\t' % (saved+epoch+1, time.time()-start_time, ttl_loss/item_count))\n",
        "        train_loss=np.append(train_loss, ttl_loss.detach().cpu().numpy()/item_count)\n",
        "        nms_confidence, nms_box = non_maximum_suppression(pred_confidence[0].detach().cpu().numpy(), pred_box[0].detach().cpu().numpy(), boxs_default)\n",
        "        visualize(0,\"train\", pred_confidence, pred_box, ann_confidence_, ann_box_, images_, boxs_default)\n",
        "        visualize(0,\"train\", nms_confidence, nms_box, ann_confidence_, ann_box_, images_, boxs_default)\n",
        "\n",
        "        if epoch%2==1:\n",
        "            torch.save(network.state_dict(), cpts + str(saved+epoch+1)+ext)\n",
        "\n",
        "        network.eval()\n",
        "        ttl_loss=0\n",
        "        item_count=0\n",
        "    \n",
        "        for i, data in enumerate(dataloader_test, 0):\n",
        "            images_, ann_box_, ann_confidence_, shape, _ = data\n",
        "            images = images_.to(device)\n",
        "            ann_box = ann_box_.to(device)\n",
        "            ann_confidence = ann_confidence_.to(device)\n",
        "            pred_confidence, pred_box = network(images)\n",
        "            loss_net=SSD_loss(pred_confidence,pred_box,ann_confidence,ann_box)\n",
        "            ttl_loss+=loss_net.data\n",
        "            item_count+=1\n",
        "            print(\"\\rTesting: %d\\t\\t\\t\" % (i+1), end='')\n",
        "\n",
        "        print('\\r[%d] time: %f \\tvalidation loss: %f\\t\\t\\t' % (saved+epoch+1, time.time()-start_time, ttl_loss/item_count))\n",
        "        validation_loss=np.append(validation_loss, ttl_loss.detach().cpu().numpy()/item_count)\n",
        "        nms_confidence, nms_box = non_maximum_suppression(pred_confidence[0].detach().cpu().numpy(), pred_box[0].detach().cpu().numpy(), boxs_default)\n",
        "        visualize(0, \"test\", pred_confidence, pred_box, ann_confidence_, ann_box_, images_, boxs_default)\n",
        "        visualize(0,\"train\", nms_confidence, nms_box, ann_confidence_, ann_box_, images_, boxs_default)\n",
        "\n",
        "    x_axis = np.arange(num_epochs) + 1\n",
        "    plt.plot(x_axis,train_loss,'b.-')\n",
        "    plt.plot(x_axis,validation_loss,'r.-')\n",
        "    plt.ylabel('average error')\n",
        "    plt.legend(['Training loss','Validation loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()\n",
        "\n",
        "else:    \n",
        "    test_batch_size = 1\n",
        "    if _dir=='test':\n",
        "        dataset_test = COCO(\"data/\"+_dir+\"/images/\", None, class_num, boxs_default, train = False, augmentation = False, split = False, image_size=320)\n",
        "    else:\n",
        "        dataset_test = COCO(\"data/\"+_dir+\"/images/\", \"data/\"+_dir+\"/annotations/\", class_num, boxs_default, train = False, augmentation = False, split = False, image_size=320)\n",
        "    \n",
        "    \n",
        "    dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
        "    \n",
        "    if os.path.exists(_checkpoint):\n",
        "        network.load_state_dict(torch.load(_checkpoint,map_location=torch.device(device)))\n",
        "        print(\"Loading \"+_checkpoint)\n",
        "    else:\n",
        "        print(\"Error: Loading failed.\")\n",
        "        sys.exit()\n",
        "    \n",
        "    network.eval()\n",
        "    \n",
        "    _result = _result+_dir+'/'\n",
        "    if not os.path.exists(_result):\n",
        "        os.makedirs(_result)\n",
        "    \n",
        "    for i, data in enumerate(dataloader_test, 0):\n",
        "        images_, ann_box_, ann_confidence_, shape, filename = data\n",
        "        images = images_.to(device)\n",
        "        ann_box = ann_box_.to(device)\n",
        "        ann_confidence = ann_confidence_.to(device)\n",
        "        \n",
        "        pred_confidence, pred_box = network(images)\n",
        "\n",
        "        nms_confidence, nms_box = non_maximum_suppression(pred_confidence[0].detach().cpu().numpy(), pred_box[0].detach().cpu().numpy(), boxs_default)\n",
        "\n",
        "        if _dir=='test':\n",
        "            toTxt(False,filename,i,nms_confidence, nms_box, shape, test_batch_size, boxs_default)\n",
        "        else:\n",
        "            toTxt(True,i,nms_confidence, nms_box, shape, test_batch_size, boxs_default)\n",
        "        visualize(0, _dir, pred_confidence, pred_box, ann_confidence_, ann_box_, images_, boxs_default, toFile=True, pathname=_result+str(i))\n",
        "        visualize(0, _dir, nms_confidence, nms_box, ann_confidence_, ann_box_, images_, boxs_default, toFile=True, pathname=_result+str(i)+'nms')\n",
        "        \n",
        "        print('\\rTesting: %d\\t' % (i), end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "5662bc4G0o50",
        "outputId": "977c92a6-51d4-4994-b710-b29288813733"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-d2847eb04e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train/images/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/train/annotations/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxs_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train/images/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/train/annotations/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxs_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9b13085398b1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, imgdir, anndir, class_num, boxs_default, train, augmentation, split, image_size)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mttl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttl\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 Score"
      ],
      "metadata": {
        "id": "44Womgh8L0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_gt =0\n",
        "num_pred = 0\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "def line2data(line):\n",
        "    obj = line.split()\n",
        "    cls = int(obj[0])\n",
        "    x = float(obj[1])\n",
        "    y = float(obj[2])\n",
        "    w = float(obj[3])\n",
        "    h = float(obj[4])\n",
        "    return cls,x,y,w,h\n",
        "\n",
        "def IoU(x1,y1,w1,h1,x2,y2,w2,h2):\n",
        "    X1=max(x1,x2)\n",
        "    Y1=max(y1,y2)\n",
        "    X2=min(x1+w1,x2+w2)\n",
        "    Y2=min(y1+h1,y2+h2)\n",
        "    if X1>=X2 or Y1>=Y2:\n",
        "        return 0\n",
        "    else:\n",
        "        intersection=(X2-X1)*(Y2-Y1)\n",
        "        s1=w1*h1\n",
        "        s2=w2*h2\n",
        "        return intersection/(s1+s2-intersection)\n",
        "\n",
        "ann = sorted(os.listdir('data/train/annotations'))\n",
        "pred = sorted(os.listdir('predicted_boxes_old_150ep/train'))\n",
        "val_ann = ann[round(len(ann)*0.9):len(ann)]\n",
        "val_pred = pred[round(len(pred)*0.9):len(pred)]\n",
        "\n",
        "for i in np.arange(len(val_ann)):\n",
        "    with open('data/train/annotations/'+val_ann[i]) as f:\n",
        "        lines_ann = f.readlines()\n",
        "    with open('predicted_boxes_old_150ep/train/'+val_pred[i]) as g:\n",
        "        lines_pred = g.readlines()\n",
        "    num_gt+=len(lines_ann)\n",
        "    num_pred+=len(lines_pred)\n",
        "    for j in np.arange(len(lines_pred)):\n",
        "        cls_pred,x_pred,y_pred,w_pred,h_pred=line2data(lines_pred[j])\n",
        "        t_p =False\n",
        "        for k in np.arange(len(lines_ann)):\n",
        "            cls_ann,x_ann,y_ann,w_ann,h_ann=line2data(lines_ann[k])\n",
        "            if cls_ann==cls_pred and IoU(x_ann,y_ann,w_ann,h_ann,x_pred,y_pred,w_pred,h_pred)>=0.5:\n",
        "                t_p =True\n",
        "        if t_p==True:\n",
        "            TP+=1\n",
        "        else:\n",
        "            FP+=1\n",
        "    for j in np.arange(len(lines_ann)):\n",
        "        cls_ann,x_ann,y_ann,w_ann,h_ann=line2data(lines_ann[j])\n",
        "        f_n=True\n",
        "        for k in np.arange(len(lines_pred)):\n",
        "            cls_pred,x_pred,y_pred,w_pred,h_pred=line2data(lines_pred[k])\n",
        "            if cls_ann==cls_pred and IoU(x_ann,y_ann,w_ann,h_ann,x_pred,y_pred,w_pred,h_pred)>=0.5:\n",
        "                f_n=False\n",
        "        if f_n==True:\n",
        "            FN+=1\n",
        "\n",
        "precision=TP/(TP+FP)\n",
        "recall=TP/(TP+FN)\n",
        "F1_score=2*precision*recall/(precision+recall)\n",
        "print(F1_score)\n",
        "precision=TP/num_pred\n",
        "recall=TP/num_gt\n",
        "F1_score=2*precision*recall/(precision+recall)\n",
        "print(F1_score)\n",
        "print(TP+FN)\n",
        "print(num_gt)\n",
        "print(TP+FP)\n",
        "print(num_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfYYutIn03o3",
        "outputId": "8b6bfb7a-7122-4ffd-e70f-5ad4bc26d22d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6903553299492386\n",
            "0.6888567293777135\n",
            "692\n",
            "695\n",
            "687\n",
            "687\n"
          ]
        }
      ]
    }
  ]
}